{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Crowd Counting using YOLOv3\n",
        "\n",
        "YOLOv3 (You Only Look Once) is an object detection model. Its goal is to identify and classify discrete objects in an image, such as cars, people, or animals, and draw bounding boxes around them. YOLO is designed to detect objects with well-de\u0000ned boundaries. It works best when the objects are relatively large, clearly separated, and the number of objects is small or moderate.\n"
      ],
      "metadata": {
        "id": "6FVOM1mIaeNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import  cv2_imshow\n",
        "# Load YOLO model\n",
        "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "# Load the video\n",
        "cap = cv2.VideoCapture('/content/Drone Footage of Canberra s HISTORIC Crowd.mp4')\n",
        "# Time frame from 0:14 to 0:32 corresponds roughly to frames 14*fps to 32*fps\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "start_time = 14  # Start time in seconds\n",
        "end_time = 32    # End time in seconds\n",
        "start_frame = int(start_time * fps)\n",
        "end_frame = int(end_time * fps)\n",
        "frame_count = 0\n",
        "people_count = []\n",
        "# Move to the starting frame\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "break\n",
        "    frame_count += 1\n",
        "    current_frame = start_frame + frame_count\n",
        "    # Stop if the current frame is beyond the end frame\n",
        "    if current_frame > end_frame:\n",
        "break\n",
        "    # Prepare the frame for YOLO\n",
        "    height, width, channels = frame.shape\n",
        "\n",
        "https://colab.research.google.com/drive/1iZSXW37ujBmmsHJXRECMDXFPdWHyRhp0#scrollTo=d8pkDQsaHtF-&printMode=true 1/191\n",
        "9/19/24, 4:11 PM lauretta.ipynb - Colab\n",
        "     blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=F\n",
        "    net.setInput(blob)\n",
        "    outs = net.forward(output_layers)\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    # Processing the output\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5 and class_id == 0:  # Class 0 is 'person' in COCO da\n",
        "                # Object detected\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "                # Rectangle coordinates\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "    # Apply Non-Maximum Suppression to remove redundant boxes\n",
        "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "    # Count the number of people\n",
        "    count = len(indexes)\n",
        "    people_count.append(count)\n",
        "    # Draw bounding boxes (Optional, to visualize)\n",
        "    for i in range(len(boxes)):\n",
        "        if i in indexes:\n",
        "            x, y, w, h = boxes[i]\n",
        "            label = str('Person')\n",
        "            color = (0, 255, 0)\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color,\n",
        "    # Display the frame (Optional)\n",
        "    cv2_imshow(frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "https://colab.research.google.com/drive/1iZSXW37ujBmmsHJXRECMDXFPdWHyRhp0#scrollTo=d8pkDQsaHtF-&printMode=true 2/191\n",
        "\n",
        "9/19/24, 4:11 PM lauretta.ipynb - Colab\n",
        "# Average crowd size\n",
        "estimated_crowd_size = np.mean(people_count)\n",
        "print(f\"Estimated Crowd Size: {estimated_crowd_size:.2f}\")"
      ],
      "metadata": {
        "id": "T321_RHOaa55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YoloV3 has a harder time detecting small, distant objects (e.g., people far away) because its bounding box detectors work better with objects that occupy a significant portion of the image. In crowd scenes, many people can appear as very small, distant figures, making it difficult for YOLOv3 to detect them reliably.\n",
        "\n",
        "YOLOv3 is optimized for real-time object detection. It is fast and can process frames quickly, but it sacrifices accuracy in dense or overlapping scenarios, especially when the number of objects increases. It is designed for tasks like autonomous driving, general object detection, and real-time monitoring."
      ],
      "metadata": {
        "id": "9l-aB6NbXN8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crowd Counting using CSRNet Model\n",
        "\n",
        "CSRNet (Convolutional Neural Network for Crowd Counting) is a crowd density estimation model. Instead of detecting individual objects with bounding boxes, CSRNet generates density maps that estimate how many people are present in various parts of the image. It is specialized for crowd counting, especially in dense scenes, where people overlap and are closely packed together.\n",
        "\n",
        "CSRNet is designed specifically to estimate the number of people in dense crowds, while YOLOv3 is designed to detect individual objects, which becomes difficult when people are packed closely together.\n",
        "\n",
        "Excels in dense crowd situations because it doesn't rely on individual detection. Instead, CSRNet produces a density map where each pixel represents a \"contribution\" to the total count, allowing it to estimate the number of people even when they overlap or are partially occluded.\n",
        "\n",
        "YOLOv3 struggles with high-density, overlapping crowds because it is focused on detecting distinct objects. CSRNet, on the other hand, handles overlapping and occluded people well because it focuses on generating a density map for estimation rather than individual detection.\n",
        "\n"
      ],
      "metadata": {
        "id": "mJuGWQZJWo1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class CSRNet(nn.Module):\n",
        "    def __init__(self, load_weights=False):\n",
        "        super(CSRNet, self).__init__()\n",
        "        self.seen = 0\n",
        "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
        "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
        "        self.frontend = make_layers(self.frontend_feat)\n",
        "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "        if not load_weights:\n",
        "            mod = models.vgg16(pretrained = True)\n",
        "            self._initialize_weights()\n",
        "            for i in range(len(self.frontend.state_dict().items())):\n",
        "                list(self.frontend.state_dict().items())[i][1].data[:] = list(mod.state_dict().items())[i][1].data[:]\n",
        "    def forward(self,x):\n",
        "        x = self.frontend(x)\n",
        "        x = self.backend(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
        "    if dilation:\n",
        "        d_rate = 2\n",
        "    else:\n",
        "        d_rate = 1\n",
        "    layers = []\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "m7fY8IQzMwaq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import scipy.io as io\n",
        "import PIL.Image as Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt, cm as c\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import scipy\n",
        "import torchvision.transforms.functional as F\n",
        "import torch\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BrAP3uHOM6OS",
        "outputId": "b3d087e3-92d3-4de5-b1a1-d3076924a228"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-815c7c4b4df0>:6: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CSRNet()\n",
        "\n",
        "checkpoint = torch.load('weights.pth', map_location=\"cpu\")\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()\n",
        "\n",
        "transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]),\n",
        "                   ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o3AB9c24ND8J",
        "outputId": "14757aa0-5000-4d73-8a94-4c8205d664cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-27-5c863132dbcc>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('weights.pth', map_location=\"cpu\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get FPS and calculate frame range for 0:14s to 0:32s\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    start_time = 14  # 14 seconds\n",
        "    end_time = 32    # 32 seconds\n",
        "    duration = end_time - start_time  # Duration in seconds\n",
        "\n",
        "    # Total number of frames in the time range\n",
        "    total_frames = int(fps * duration)\n",
        "\n",
        "    # Select 18 evenly spaced frames from the time range\n",
        "    frames_to_capture = np.linspace(0, total_frames, 18, endpoint=False, dtype=int)\n",
        "\n",
        "    # Set the video to the start time\n",
        "    start_frame = int(fps * start_time)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "    total_crowd_count = 0\n",
        "    frame_count = 0\n",
        "\n",
        "    for frame_index in frames_to_capture:\n",
        "        # Move to the desired frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame + frame_index)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Convert frame to PIL Image\n",
        "        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Preprocess the image\n",
        "        img = transform(pil_img).unsqueeze(0)\n",
        "\n",
        "        # Make the prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(img)\n",
        "            predicted_count = int(output.detach().cpu().sum().numpy())\n",
        "\n",
        "        # Accumulate the total crowd count\n",
        "        total_crowd_count += predicted_count\n",
        "\n",
        "        # Convert the density map to a numpy array for visualization\n",
        "        density_map = output.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Normalize the density map for visualization\n",
        "        density_map_normalized = (density_map - density_map.min()) / (density_map.max() - density_map.min())\n",
        "        density_map_colored = cm.jet(density_map_normalized)[:, :, :3]  # Apply colormap\n",
        "\n",
        "        # Resize the density map to match the original frame size\n",
        "        density_map_resized = cv2.resize(density_map_colored, (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "        # Overlay the density map on the original frame\n",
        "        overlay = cv2.addWeighted(frame, 0.6, (density_map_resized * 255).astype(np.uint8), 0.4, 0)\n",
        "\n",
        "        # Show the crowd count on the frame\n",
        "        cv2.putText(overlay, f\"Crowd Count: {predicted_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        # Show the frame with the density map overlay\n",
        "        cv2_imshow(overlay)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # Output the total crowd count for the 18 frames\n",
        "    print(f\"Total Crowd Count for 18 frames between 0.14s and 0.32s: {total_crowd_count}\")\n",
        "    avg_crowd_count = total_crowd_count / frame_count\n",
        "    print(f\"Average Crowd Count: {avg_crowd_count:.2f}\")\n",
        "\n",
        "# Example usage:\n",
        "process_video('/content/Drone Footage of Canberra s HISTORIC Crowd.mp4')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WlEGgkdoNqXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outputs a density map. In a crowd density map, each pixel contributes a fractional count to the total crowd size, making it very effective for counting people in images where individuals are not easily separable, such as in large or dense crowds.\n",
        "\n",
        "CSRNet is designed to output density maps that work better for crowd counting tasks, while YOLOv3's bounding box output is not well suited for estimating the number of people in crowded scenes."
      ],
      "metadata": {
        "id": "UpsyIYltXAQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Key Reasons Why CSRNet Works Better for Crowd Counting:**\n",
        "\n",
        "\n",
        "Crowd density map: CSRNet is designed to produce a density map, making it far more effective for estimating the number of people in densely packed scenes.\n",
        "Scale and occlusion handling: CSRNet handles varying scales (people far away or close) and occlusions better than YOLOv3.\n",
        "Not reliant on bounding boxes: CSRNet doesn’t need to detect each individual person as a distinct object, whereas YOLOv3 struggles with overlapping or occluded objects.\n",
        "Task-specific design: CSRNet is specifically designed for crowd counting, while YOLOv3 is a general object detector, making CSRNet more accurate in this specialized task.\n",
        "In conclusion, CSRNet is better suited for crowd counting tasks, particularly in dense crowds, due to its specialized architecture that focuses on generating density maps rather than relying on object detection like YOLOv3."
      ],
      "metadata": {
        "id": "A0ngPtQ1XIIV"
      }
    }
  ]
}